Using device: cpu

============================================================
STEP 4: TRAIN SNPE (Neural Posterior Estimation)
============================================================

1. Loading split dataset...
   theta_train shape: torch.Size([427, 2])
   x_train shape: torch.Size([427, 2762])
   theta_val shape: torch.Size([107, 2])
   x_val shape: torch.Size([107, 2762])

   Dataset summary:
   - Training samples: 427
   - Validation samples: 107
   - Input dimension (frequencies): 2762
   - Parameter dimension: 2 (xHI, fX)

2. Defining prior...
   Prior: xHI ∈ [0, 1], fX ∈ [-4, 1]

3. Initializing SNPE...

4. Training with IMPROVED hyperparameters:
   batch_size: 32 (was 16)
   learning_rate: 0.0005 (was 1e-4)
   num_epochs: 300 (was 100)
   → Expected: SMOOTHER, well-defined posteriors

5. Training neural posterior...
   This will take 30-60 minutes on CPU...
   Improved hyperparameters should yield better posteriors...

✅ Training complete!

6. Saving posterior...
   Saved to: /user1/supriyo/ml_project/SBI_step_by_step/UGMRT_500h/Outputs/posterior_snpe.pt
   Metadata saved to: /user1/supriyo/ml_project/SBI_step_by_step/UGMRT_500h/Outputs/metadata_train.pkl

============================================================
✅ STEP 4 COMPLETE - IMPROVED TRAINING!
============================================================

Training Summary:
  - Trained on: 427 samples
  - Validated on: 107 samples
  - Model saved: /user1/supriyo/ml_project/SBI_step_by_step/UGMRT_500h/Outputs/posterior_snpe.pt
  - Hyperparameters: IMPROVED for smooth posteriors
  - batch_size: 32, lr: 0.0005, epochs: 300

Expected results:
  - Smoother contours in posterior distributions
  - Better-defined 1σ and 2σ credible intervals
  - Professional appearance similar to paper figures

Next: Run Step 5 for posterior inference on 5 test points

"""
Step 4b: Evaluate Trained Posterior (Check training quality)
Check if the posterior was trained well using validation set
"""

import numpy as np
import pickle
import torch
import os
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_squared_error

# Setup paths
DATA_DIR = "/user1/supriyo/ml_project/SBI_step_by_step/UGMRT_500h/Data/Train_test_data/"
OUTPUT_DIR = "/user1/supriyo/ml_project/SBI_step_by_step/UGMRT_500h/Outputs/"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Setup logging
log_file = os.path.join(OUTPUT_DIR, "evaluation_log.txt")
log_fp = open(log_file, "w")

def log_print(msg):
    """Print to both console and log file"""
    print(msg)
    log_fp.write(msg + "\n")
    log_fp.flush()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

log_print("="*60)
log_print("STEP 4b: EVALUATE TRAINED POSTERIOR")
log_print("="*60)

# Load validation data
log_print("\n1. Loading validation data...")
split_path = os.path.join(DATA_DIR, "train_val_test_split.pkl")
with open(split_path, "rb") as f:
    split_data = pickle.load(f)

theta_val = split_data["theta_val"]  # (107, 2)
x_val = torch.from_numpy(split_data["x_1d_val"]).float().to(device)  # (107, 2762)

log_print(f"   Validation set size: {theta_val.shape[0]}")
log_print(f"   theta_val shape: {theta_val.shape}")
log_print(f"   x_val shape: {x_val.shape}")

# Load trained posterior
log_print("\n2. Loading trained posterior...")
posterior_path = os.path.join(OUTPUT_DIR, "posterior_snpe.pt")
posterior = torch.load(posterior_path, map_location=device)
log_print(f"   Loaded from: {posterior_path}")

# Sample from posterior on validation set
log_print("\n3. Sampling from posterior on validation set...")
n_samples = 500  # Fewer samples for evaluation speed
log_print(f"   Drawing {n_samples} samples per observation...")

posterior_means = []
posterior_stds = []

for i in range(len(theta_val)):
    if (i+1) % 20 == 0:
        log_print(f"   Processed {i+1}/{len(theta_val)}")
    
    # Sample from posterior
    # posterior.sample() needs x as condition
    x_condition = x_val[i:i+1]  # (1, 2762)
    
    try:
        samples = posterior.sample((n_samples,), x=x_condition)
    except TypeError:
        # Alternative API
        samples = posterior.sample((n_samples,), condition=x_condition)
    
    if isinstance(samples, torch.Tensor):
        samples = samples.detach().cpu().numpy()
    samples = samples.reshape(-1, 2)  # Ensure shape (n_samples, 2)
    
    mean = samples.mean(axis=0)
    std = samples.std(axis=0)
    
    posterior_means.append(mean)
    posterior_stds.append(std)

posterior_means = np.array(posterior_means)  # (107, 2)
posterior_stds = np.array(posterior_stds)    # (107, 2)

log_print(f"\n   Posterior means shape: {posterior_means.shape}")
log_print(f"   Posterior stds shape: {posterior_stds.shape}")

# Calculate metrics
log_print("\n4. Computing evaluation metrics...")

# R2 Score for each parameter
r2_xhi = r2_score(theta_val[:, 0], posterior_means[:, 0])
r2_fx = r2_score(theta_val[:, 1], posterior_means[:, 1])

# RMSE for each parameter
rmse_xhi = np.sqrt(mean_squared_error(theta_val[:, 0], posterior_means[:, 0]))
rmse_fx = np.sqrt(mean_squared_error(theta_val[:, 1], posterior_means[:, 1]))

# Z-score (normalized residuals)
z_scores_xhi = (theta_val[:, 0] - posterior_means[:, 0]) / (posterior_stds[:, 0] + 1e-8)
z_scores_fx = (theta_val[:, 1] - posterior_means[:, 1]) / (posterior_stds[:, 1] + 1e-8)

z_mean_xhi = z_scores_xhi.mean()
z_std_xhi = z_scores_xhi.std()
z_mean_fx = z_scores_fx.mean()
z_std_fx = z_scores_fx.std()

log_print("\n" + "="*60)
log_print("EVALUATION RESULTS")
log_print("="*60)

log_print("\nxHI (Neutral Fraction) Metrics:")
log_print(f"  R¬≤ Score: {r2_xhi:.4f} (ideal: 1.0, bad: <0.5)")
log_print(f"  RMSE: {rmse_xhi:.6f} (lower is better)")
log_print(f"  Z-score mean: {z_mean_xhi:.4f} (ideal: 0.0)")
log_print(f"  Z-score std:  {z_std_xhi:.4f} (ideal: 1.0)")

log_print("\nfX (log10 X-ray heating) Metrics:")
log_print(f"  R¬≤ Score: {r2_fx:.4f} (ideal: 1.0, bad: <0.5)")
log_print(f"  RMSE: {rmse_fx:.6f} (lower is better)")
log_print(f"  Z-score mean: {z_mean_fx:.4f} (ideal: 0.0)")
log_print(f"  Z-score std:  {z_std_fx:.4f} (ideal: 1.0)")

# Overall assessment
log_print("\n" + "="*60)
log_print("TRAINING QUALITY ASSESSMENT")
log_print("="*60)

quality_score = 0
assessments = []

# Check R2
if r2_xhi > 0.8 and r2_fx > 0.8:
    assessments.append("‚úÖ R¬≤ Scores: EXCELLENT (>0.8)")
    quality_score += 3
elif r2_xhi > 0.6 and r2_fx > 0.6:
    assessments.append("‚úÖ R¬≤ Scores: GOOD (>0.6)")
    quality_score += 2
elif r2_xhi > 0.3 and r2_fx > 0.3:
    assessments.append("‚ö†Ô∏è  R¬≤ Scores: FAIR (>0.3)")
    quality_score += 1
else:
    assessments.append("‚ùå R¬≤ Scores: POOR (<0.3)")

# Check Z-scores
if abs(z_mean_xhi) < 0.2 and abs(z_mean_fx) < 0.2:
    assessments.append("‚úÖ Z-score means: CENTERED (close to 0)")
    quality_score += 3
elif abs(z_mean_xhi) < 0.5 and abs(z_mean_fx) < 0.5:
    assessments.append("‚úÖ Z-score means: REASONABLE")
    quality_score += 2
else:
    assessments.append("‚ö†Ô∏è  Z-score means: SHIFTED (far from 0)")
    quality_score += 1

if 0.8 < z_std_xhi < 1.2 and 0.8 < z_std_fx < 1.2:
    assessments.append("‚úÖ Z-score std: CALIBRATED (close to 1)")
    quality_score += 3
elif 0.5 < z_std_xhi < 1.5 and 0.5 < z_std_fx < 1.5:
    assessments.append("‚úÖ Z-score std: ACCEPTABLE")
    quality_score += 2
else:
    assessments.append("‚ö†Ô∏è  Z-score std: MISCALIBRATED")
    quality_score += 1

for assessment in assessments:
    log_print(f"\n{assessment}")

log_print(f"\n{'='*60}")
if quality_score >= 8:
    log_print("üéâ OVERALL: TRAINING IS GOOD! Ready for inference")
elif quality_score >= 5:
    log_print("‚úÖ OVERALL: TRAINING IS ACCEPTABLE. Proceed cautiously")
else:
    log_print("‚ö†Ô∏è  OVERALL: TRAINING MAY NEED IMPROVEMENT. Consider retraining")
log_print(f"{'='*60}")

# Create visualization
log_print("\n5. Creating evaluation plots...")

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot 1: True vs Predicted xHI
axes[0, 0].scatter(theta_val[:, 0], posterior_means[:, 0], alpha=0.6, s=50)
axes[0, 0].errorbar(theta_val[:, 0], posterior_means[:, 0], 
                    yerr=posterior_stds[:, 0], fmt='none', alpha=0.3, elinewidth=1)
axes[0, 0].plot([0, 1], [0, 1], 'r--', label='Perfect', linewidth=2)
axes[0, 0].set_xlabel('True xHI', fontsize=11)
axes[0, 0].set_ylabel('Posterior Mean xHI', fontsize=11)
axes[0, 0].set_title(f'xHI: R¬≤={r2_xhi:.3f}', fontweight='bold')
axes[0, 0].grid(alpha=0.3)
axes[0, 0].legend()

# Plot 2: True vs Predicted fX
axes[0, 1].scatter(theta_val[:, 1], posterior_means[:, 1], alpha=0.6, s=50, color='orange')
axes[0, 1].errorbar(theta_val[:, 1], posterior_means[:, 1], 
                    yerr=posterior_stds[:, 1], fmt='none', alpha=0.3, elinewidth=1)
axes[0, 1].plot([-4, 1], [-4, 1], 'r--', label='Perfect', linewidth=2)
axes[0, 1].set_xlabel('True fX', fontsize=11)
axes[0, 1].set_ylabel('Posterior Mean fX', fontsize=11)
axes[0, 1].set_title(f'fX: R¬≤={r2_fx:.3f}', fontweight='bold')
axes[0, 1].grid(alpha=0.3)
axes[0, 1].legend()

# Plot 3: Z-score distribution xHI
axes[1, 0].hist(z_scores_xhi, bins=20, edgecolor='black', alpha=0.7, color='steelblue')
axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2, label='Mean')
axes[1, 0].set_xlabel('Z-score', fontsize=11)
axes[1, 0].set_ylabel('Count', fontsize=11)
axes[1, 0].set_title(f'Z-score xHI: Œº={z_mean_xhi:.3f}, œÉ={z_std_xhi:.3f}', fontweight='bold')
axes[1, 0].grid(alpha=0.3)
axes[1, 0].legend()

# Plot 4: Z-score distribution fX
axes[1, 1].hist(z_scores_fx, bins=20, edgecolor='black', alpha=0.7, color='coral')
axes[1, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Mean')
axes[1, 1].set_xlabel('Z-score', fontsize=11)
axes[1, 1].set_ylabel('Count', fontsize=11)
axes[1, 1].set_title(f'Z-score fX: Œº={z_mean_fx:.3f}, œÉ={z_std_fx:.3f}', fontweight='bold')
axes[1, 1].grid(alpha=0.3)
axes[1, 1].legend()

plt.tight_layout()
eval_plot_path = os.path.join(OUTPUT_DIR, "evaluation_metrics.png")
plt.savefig(eval_plot_path, dpi=150, bbox_inches='tight')
log_print(f"   Plot saved to: {eval_plot_path}")
plt.close()

# Save metrics to file
metrics_dict = {
    "r2_xhi": r2_xhi,
    "r2_fx": r2_fx,
    "rmse_xhi": rmse_xhi,
    "rmse_fx": rmse_fx,
    "z_mean_xhi": z_mean_xhi,
    "z_std_xhi": z_std_xhi,
    "z_mean_fx": z_mean_fx,
    "z_std_fx": z_std_fx,
    "quality_score": quality_score,
}

metrics_path = os.path.join(OUTPUT_DIR, "evaluation_metrics.pkl")
with open(metrics_path, "wb") as f:
    pickle.dump(metrics_dict, f)
log_print(f"   Metrics saved to: {metrics_path}")

log_print("\n" + "="*60)
log_print("‚úÖ EVALUATION COMPLETE!")
log_print("="*60)

# Close log
log_fp.close()
print(f"\n‚úÖ Log saved to: {log_file}")
